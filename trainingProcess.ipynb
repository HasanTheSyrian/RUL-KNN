{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This document shows the training process of the model.\n",
    "\n",
    "Reference for time to train is a 16'' Apple M1 Pro 10 Core 16 GB RAM Machine.\n",
    "\n",
    "Comments are available only the first time a specific line of code is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1\n",
    "\n",
    "### Algorithm: KNN\n",
    "\n",
    "### Paramters:\n",
    "K_Neighbords: 5\n",
    "\n",
    "### Results:\n",
    "\n",
    "MSE: 2660.73\n",
    "R^2: 0.974\n",
    "\n",
    "Time to train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "bat = pd.read_csv('Battery_RUL.csv')\n",
    "\n",
    "#remove \"Cycle_Index\" since it doesn't carry info and \"RUL\" from X since its the objective\n",
    "X = bat.iloc[:, 1:-1]\n",
    "y = bat[\"RUL\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4333)\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred=knn.predict(X_test)\n",
    "\n",
    "print(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "#y_pred = pd.DataFrame(y_pred)\n",
    "#y_pred.to_csv(\"/test.csv\")\n",
    "\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2\n",
    "\n",
    "### Algorithm: XGBoost\n",
    "\n",
    "### Paramters:\n",
    "N_estimators: 100\n",
    "Learning Rate: 0.5\n",
    "Max Depth: 10\n",
    "Random State: 4333\n",
    "\n",
    "### Results:\n",
    "\n",
    "MSE: 844.4\n",
    "R^2: 0.991\n",
    "\n",
    "Time to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "bat = pd.read_csv('Battery_RUL.csv')\n",
    "\n",
    "#remove \"Cycle_Index\" since it doesn't carry info and \"RUL\" from X since its the objective\n",
    "X = bat.iloc[:, 1:-1]\n",
    "y = bat[\"RUL\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4333)\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.5, max_depth=10, random_state=4333)\n",
    "\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "print(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3\n",
    "\n",
    "### Algorithm: XGBoost + GridSearch\n",
    "\n",
    "### Paramters:\n",
    "```py\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 2, 'subsample': 1.0}\n",
    "MSE: 766.1\n",
    "R^2: 0.992\n",
    "\n",
    "Time to train: 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "bat = pd.read_csv('Battery_RUL.csv')\n",
    "\n",
    "X = bat.iloc[:, 1:-1]\n",
    "y = bat[\"RUL\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4333)\n",
    "\n",
    "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', random_state=4333)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_regressor = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_xgb_regressor.predict(X_test)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Mean Squared Error: \", metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared: \", metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 4\n",
    "\n",
    "### Algorithms & Techniques: XGBoost, GridSearch\n",
    "\n",
    "### Paramters:\n",
    "```py\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 100],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "```\n",
    "*same code but increase max \"max_depth\" parameter to 100\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 100, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 1, 'subsample': 0.8}\n",
    "MSE: 669.1\n",
    "R^2: 0.993\n",
    "\n",
    "Time to train: 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 5\n",
    "\n",
    "### Algorithms & Techniques: XGBoost, GridSearch and Normalization\n",
    "\n",
    "### Paramters:\n",
    "```py\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 100],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 100, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 2, 'subsample': 1.0}\n",
    "MSE: 1515.7\n",
    "R^2: 0.985\n",
    "\n",
    "Time to train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "bat = pd.read_csv('Battery_RUL.csv')\n",
    "\n",
    "X = bat.iloc[:, 1:-1]\n",
    "y = bat[\"RUL\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4333)\n",
    "\n",
    "#normalize data\n",
    "X_train = sklearn.preprocessing.normalize(X_train)\n",
    "\n",
    "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', random_state=4333)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_regressor = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_xgb_regressor.predict(X_test)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Mean Squared Error: \", metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared: \", metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 6\n",
    "\n",
    "### Algorithms & Techniques: Random Forest, GridSearch\n",
    "\n",
    "### Paramters:\n",
    "```py\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 100],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_impurity_decrease': [0, 0.1, 0.2],\n",
    "    'min_weight_fraction_leaf': [0, 0.1, 0.2],\n",
    "    'random_state': [100, 500, 4000]\n",
    "}\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  \n",
    "MSE: 835.6\n",
    "R^2: 0.991\n",
    "\n",
    "Time to train: 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.ensemble\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "bat = pd.read_csv('Battery_RUL.csv')\n",
    "\n",
    "X = bat.iloc[:, 1:-1]\n",
    "y = bat[\"RUL\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4333)\n",
    "\n",
    "# Intitialize the RandomForest Regressor\n",
    "randomForest_regressor = sklearn.ensemble.RandomForestRegressor(criterion='squared_error')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 100],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_impurity_decrease': [0, 0.1, 0.2],\n",
    "    'min_weight_fraction_leaf': [0, 0.1, 0.2],\n",
    "    'random_state': [100, 500, 4000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=randomForest_regressor, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_xgb_regressor = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_xgb_regressor.predict(X_test)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Mean Squared Error: \", metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared: \", metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 7\n",
    "\n",
    "### Algorithms & Techniques: XGBoost, GridSearch, and Paramter Optimization\n",
    "\n",
    "The best result so far uses XGBoost, looking at that result's parameters; if the best value for a parameter is the greatest number, I set it as the lower limit and add two greater parameters.\n",
    "\n",
    "**note: This time random state gets its own array instead of just using 4333.**\n",
    "\n",
    "**note: It took 1 hour and 45 minutes to train a slightly worse model compared to one that took 5 minutes to train.**\n",
    "\n",
    "**note: From now on the code will be the same from trail 4 only the parameters will change.**\n",
    "\n",
    "### Paramters:\n",
    "\n",
    "```py\n",
    "param_grid = {\n",
    "     'n_estimators': [200, 300, 1000],\n",
    "     'learning_rate': [0.1, 0.15, 0.19],\n",
    "     'max_depth': [100, 200, 500],\n",
    "     'subsample': [0.1, 0.5, 0.8],\n",
    "     'colsample_bytree': [0.1, 0.5, 0.8],\n",
    "     'reg_alpha': [1, 2, 3],\n",
    "     'reg_lambda': [0.05, 0.5, 0.9],\n",
    "     'random_state': [100, 500, 4000]\n",
    " }\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 100, 'n_estimators': 300, 'random_state: 500, 'reg_alpha': 3, 'reg_lambda': 0.9, 'subsample': 0.8} \n",
    "MSE: 743.6\n",
    "R^2: 0.992\n",
    "\n",
    "Time to train: 1 hour and 45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 8\n",
    "\n",
    "### Algorithms & Techniques: XGBoost, GridSearch, and Paramter Optimization\n",
    "\n",
    "Comparing the two results above and seeing the common best parameters:\n",
    "\n",
    "**found in both:**\n",
    "\n",
    "**colsample_bytree: 0.8**\n",
    "**learning rate: 0.1**\n",
    "**max depth: 100**\n",
    "**n_estimators: not common**\n",
    "**reg_alpha: not common**\n",
    "**reg_lambda: not common**\n",
    "**subsample: 0.8**\n",
    "**random_state: not common**\n",
    "\n",
    "looking at the **not common:**\n",
    "\n",
    "`for n_estimators`, 200 and 300 were best out of [50, 100, 200] [200, 300, 1000] so **[200, 250, 300] will be used**\n",
    "\n",
    "`for reg_alpha`, 1 and 3 were best out of [0, 0.1] [1, 2, 3] we can see that the maximum value was always chosen. Looking at the XGBoost documentation, the range for the paramter is [0, infin] so **[5, 10, 100] will be used**\n",
    "\n",
    "`for reg_lambda`, 1 and 0.9 were best out of [1, 1.5, 2] [0.05, 0.5, 0.9] so [1] will be used.\n",
    "\n",
    "`for random state`, 500 was best out of [100, 500, 4000, 4333] so [300, 500, 1000] will be used.\n",
    "\n",
    "### Paramters:\n",
    "\n",
    "```py\n",
    "param_grid = {\n",
    "     'n_estimators': [200, 250, 300],\n",
    "     'learning_rate': [0.1],\n",
    "     'max_depth': [100],\n",
    "     'subsample': [0.8],\n",
    "     'colsample_bytree': [0.8],\n",
    "     'reg_alpha': [5, 10, 100],\n",
    "     'reg_lambda': [1],\n",
    "     'random_state': [300, 500, 1000]\n",
    " }\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 100, 'n_estimators': 300, 'random_state: 500, 'reg_alpha': 10, 'reg_lambda': 1, 'subsample': 0.8} \n",
    "MSE: 753.8\n",
    "R^2: 0.992\n",
    "\n",
    "Time to train: 24 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 9\n",
    "\n",
    "### Algorithms & Techniques: XGBoost, GridSearch, and Paramter Optimization\n",
    "\n",
    "Comparing the results we can see that `n_esitmators` and `random_state` are the only paramters that have enough scale to try more values with. The result is close to the best result so far but not better.\n",
    "\n",
    "### Paramters:\n",
    "\n",
    "```py\n",
    "param_grid = {\n",
    "    'n_estimators': [350, 750, 900],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [100],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'reg_alpha': [1],\n",
    "    'reg_lambda': [1],\n",
    "    'random_state': [600, 700, 900]\n",
    "}\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 100, 'n_estimators': 350, 'random_state: 700, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8} \n",
    "MSE: 684.9\n",
    "R^2: 0.993\n",
    "\n",
    "Time to train: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 10\n",
    "\n",
    "### Algorithms & Techniques: XGBoost, GridSearch, and Paramter Optimization\n",
    "\n",
    "The main difference between the last trial and the best trial is `n_estimators`, best is 200 and previous is 350. Also, the best `random_state` is 700 so far. The result is very close to the best result so far but not better.\n",
    "\n",
    "### Paramters:\n",
    "\n",
    "```py\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [100],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'reg_alpha': [1],\n",
    "    'reg_lambda': [1],\n",
    "    'random_state': [700]\n",
    "}\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 100, 'n_estimators': 200, 'random_state: 700, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8} \n",
    "MSE: 686.7\n",
    "R^2: 0.993\n",
    "\n",
    "Time to train: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 11\n",
    "\n",
    "### Algorithms & Techniques: XGBoost, GridSearch, and Paramter Optimization\n",
    "\n",
    "Trying `n_estimator` as 2000 and `random_state` as 4333 gives us a new best result.\n",
    "\n",
    "### Paramters:\n",
    "\n",
    "```py\n",
    "param_grid = {\n",
    "    'n_estimators': [2000],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [100],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'reg_alpha': [1],\n",
    "    'reg_lambda': [1],\n",
    "    'random_state': [4333]\n",
    "}\n",
    "```\n",
    "\n",
    "### Results:\n",
    "\n",
    "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 100, 'n_estimators': 200, 'random_state: 4333, 'reg_alpha': 1, 'reg_lambda': 1-, 'subsample': 0.8} \n",
    "MSE: 618.3\n",
    "R^2: 0.994\n",
    "\n",
    "Time to train: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
